selenium>=4.16.0
webdriver-manager>=4.0.0
beautifulsoup4>=4.12.2
lxml>=4.9.3




def crawl_site(start_url: str, out_file: str, max_pages: int = 100, delay: float = 1.0, headless: bool = True) -> None:
	"""BFS crawl internal links starting from start_url until max_pages reached.

	Writes one JSON object per line to out_file.
	"""
	parsed_start = urlparse(start_url)
	base_netloc = parsed_start.netloc

	if not is_allowed_to_crawl(start_url):
		print("Robots.txt disallows crawling this site. Exiting.")
		return

	visited: Set[str] = set()
	q = deque([start_url])

	count = 0
	with open(out_file, "w", encoding="utf-8") as outf:
		while q and count < max_pages:
			url = q.popleft()
			if url in visited:
				continue
			print(f"Visiting ({count+1}/{max_pages}): {url}")

			# polite sleep
			time.sleep(delay)

			# Use Selenium to load the page
			driver = create_driver(headless=headless)
			try:
				driver.get(url)

				# use explicit wait for body
				try:
					WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
				except Exception:
					pass

				# Example of clicking a button to reveal content if present (assignment requirement)
				# We'll try an XPath to find a cookie accept button or similar common control
				try:
					cookie_btn = driver.find_element(By.XPATH, "//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'accept')]")
					cookie_btn.click()
					print("Clicked cookie/accept button via XPath")
				except Exception:
					# not critical; proceed
					pass

				# Scroll to bottom to trigger lazy-loaded content (optional A-level feature)
				try:
					driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
					time.sleep(0.5)
				except Exception:
					pass

				title = driver.title
				html = driver.page_source

				# Extract attributes (>=5)
				attributes = extract_attributes(url, html, title)

				# Write JSON line
				outf.write(json.dumps(attributes, ensure_ascii=False) + "\n")
				outf.flush()

				visited.add(url)
				count += 1

				# Enqueue new internal links using CSS selector (example of CSS selector usage)
				soup = BeautifulSoup(html, "lxml")
				for a in soup.select("a[href]"):
					href = a.get("href")
					if not href:
						continue
					abs_href = urljoin(url, href)
					parsed = urlparse(abs_href)
					# only follow same domain
					if parsed.netloc != base_netloc:
						continue
					# normalize scheme/netloc/path
					norm = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
					if norm not in visited and norm not in q:
						q.append(norm)

			finally:
				try:
					driver.quit()
				except Exception:
					pass

	print(f"Crawl finished. {count} pages written to {out_file}")


def create_driver(headless: bool = True) -> webdriver.Chrome:
	"""Create and return a Chrome webdriver instance using webdriver-manager."""
	chrome_options = Options()
	if headless:
		chrome_options.add_argument("--headless=new")
		chrome_options.add_argument("--disable-gpu")
	chrome_options.add_argument("--no-sandbox")
	chrome_options.add_argument("--disable-dev-shm-usage")

	# Use webdriver-manager to install the driver automatically
	service = Service(ChromeDriverManager().install())
	driver = webdriver.Chrome(service=service, options=chrome_options)
	return driver


def fetch_page(url: str, headless: bool = True, out_dir: str | Path = "screenshots", screenshot: bool = True) -> Dict[str, Any]:
	"""Open `url`, optionally take a screenshot, and return a dict with title and HTML.

	Returns: {'title': str, 'html': str, 'screenshot': <path or None>}
	"""
	out_dir = Path(out_dir)
	out_dir.mkdir(parents=True, exist_ok=True)

	driver = create_driver(headless=headless)
	try:
		driver.set_page_load_timeout(30)
		driver.get(url)

		# Wait briefly for dynamic content (wait for <body>)
		try:
			WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
		except Exception:
			# proceed even if wait times out
			pass

		title = driver.title
		html = driver.page_source

		screenshot_path = None
		if screenshot:
			host = url.split("//")[-1].split("/")[0].replace(":", "_")
			screenshot_path = out_dir / f"{host}.png"
			driver.save_screenshot(str(screenshot_path))

		return {"title": title, "html": html, "screenshot": str(screenshot_path) if screenshot_path else None}
	finally:
		driver.quit()


def scrape_html(url: str, html: str, title: str) -> Dict[str, Any]:
	"""Site-specific scrape for ohsnapmacros.com — returns only non-null fields."""
	# Reuse extract_attributes implementation for consistency
	data = extract_attributes(url, html, title)

	# add scraped timestamp where available via meta or leave out
	# (the Scrapy spider used response.headers['Date'] — with Selenium we can skip or use current time)
	return data


def main() -> None:
	parser = argparse.ArgumentParser(description="Simple Selenium starter: open a URL, save screenshot, optionally scrape HTML to JSON")
	parser.add_argument("--url", default="https://example.com", help="URL to open")
	parser.add_argument("--headless", action="store_true", help="Run browser in headless mode")
	parser.add_argument("--no-headless", dest="headless", action="store_false", help="Run with browser UI (for debugging)")
	parser.add_argument("--scrape", action="store_true", help="Parse HTML and save structured data to JSON")
	parser.add_argument("--out", default=None, help="Output JSON file for scrape results (default: <host>_scrape.json)")
	parser.add_argument("--no-screenshot", dest="screenshot", action="store_false", help="Do not save a screenshot")
	parser.add_argument("--crawl", action="store_true", help="Crawl the site (BFS) and write JSON Lines output")
	parser.add_argument("--max-pages", type=int, default=100, help="Maximum number of pages to crawl (default 100)")
	parser.add_argument("--delay", type=float, default=1.0, help="Delay between page loads in seconds (politeness) (default 1.0)")
	parser.add_argument("--out-file", default=None, help="Output JSON Lines file for crawl mode (default: <host>_crawl.jsonl)")
	parser.set_defaults(headless=True, screenshot=True)

	args = parser.parse_args()

	print(f"Opening {args.url}  (headless={args.headless})")
	result = fetch_page(args.url, headless=args.headless, out_dir=Path("screenshots"), screenshot=args.screenshot)
	print(f"Page title: {result.get('title')}")
	if result.get("screenshot"):
		print(f"Screenshot saved to {result['screenshot']}")

	if args.scrape:
		print("Scraping page HTML...")
		data = scrape_html(args.url, result.get("html", ""), result.get("title", ""))
		out_path = args.out
		if not out_path:
			host = urlparse(args.url).netloc.replace(":", "_")
			out_path = f"{host}_scrape.json"
		with open(out_path, "w", encoding="utf-8") as f:
			json.dump(data, f, ensure_ascii=False, indent=2)
		print(f"Scrape results written to {out_path}")

	if args.crawl:
		out_file = args.out_file
		if not out_file:
			host = urlparse(args.url).netloc.replace(":", "_")
			out_file = f"{host}_crawl.jsonl"
		print(f"Starting crawl of {args.url} -> {out_file} (max {args.max_pages} pages, delay {args.delay}s)")
		crawl_site(args.url, out_file=out_file, max_pages=args.max_pages, delay=args.delay, headless=args.headless)


if __name__ == "__main__":
	main()

